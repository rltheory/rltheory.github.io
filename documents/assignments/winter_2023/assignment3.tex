\documentclass{article}
\newcommand{\hwnumber}{3}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\abs}[1]{| #1 |}


\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{bbm,bm}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{xspace}
\usepackage[textsize=tiny,
]{todonotes}
\newcommand{\todot}[1]{\todo[color=blue!20!white]{T: #1}}
\newcommand{\todoc}[1]{\todo[color=orange!20!white]{Cs: #1}}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage[capitalize]{cleveref}


\usepackage{comment}

\newcommand{\cE}{\mathcal{E}}
\newcommand{\oneb}[1]{\mathbb{I}_{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cX}{\mathcal{X}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\E}{\mathbb E}
\newcommand{\bbP}{\mathbb P}
\newcommand{\V}{\mathbb V}
\renewcommand{\P}[1]{P\left\{ #1 \right\}}
\newcommand{\Prob}[1]{\mathbb{P}( #1 )}
\newcommand{\real}{\mathbb{R}}
\renewcommand{\b}[1]{\mathbf{#1}}
\newcommand{\EE}[1]{\E[#1]}
\newcommand{\bfone}{\1}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\one}[1]{\mathbb{I}\{#1\}}
\usepackage{xifthen}

\newcounter{DocPoints} 
\newcounter{QuestionPoints} 
\newcommand{\points}[1]{	\par\mbox{}\par\noindent\hfill {\bf #1 points}	\addtocounter{DocPoints}{#1}
	\addtocounter{QuestionPoints}{#1}
}
\newcommand{\tpoints}[1]{        	\ifthenelse{\isempty{#1}}	{	}	{		\addtocounter{DocPoints}{#1}
		\addtocounter{QuestionPoints}{#1}
	}													 	\par\mbox{}\par\noindent\hfill {Total: \bf \arabic{QuestionPoints}\xspace points}\par\mbox{}\par\hrule\hrule
	\setcounter{QuestionPoints}{0}
}
\newcommand{\tpoint}[1]{
	\tpoints{#1}
}

\theoremstyle{definition}
\newtheorem{question}{Question}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{solution}{Solution}
\newtheorem*{solution*}{Solution}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\excludecomment{solution}

\newcommand{\hint}{\noindent \textbf{Hint}:\xspace}


\usepackage{hyperref}

\newcommand{\epssub}{\delta}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}


\begin{document}

\begin{center}
{\Large \textbf{CMPUT 605: Theoretical Foundations of Reinforcement Learning, Winter 2023\\ Homework \#\hwnumber}}
\end{center}

\section*{Instructions}
\textbf{Submissions}
You need to submit a single PDF file, named {\tt p0\hwnumber\_<name>.pdf} where {\tt <name>} is your name.
The PDF file should include your typed up solutions (we strongly encourage to use pdf\LaTeX). 
Write your name in the title of your PDF file.
We provide a \LaTeX template that you are encouraged to use.
To submit your PDF file you should send the PDF file via private message to Vlad Tkachuk on Slack before the deadline.

\textbf{Collaboration and sources}
Work on your own. You can consult the problems with your classmates, use books
or web, papers, etc.
Also, the write-up must be your own and you must acknowledge all the
sources (names of people you worked with, books, webpages etc., including class notes.)
Failure to do so will be considered cheating.
Identical or similar write-ups will be considered cheating as well.
Students are expected to understand and explain all the steps of their proofs.

\textbf{Scheduling}
Start early: It takes time to solve the problems, as well as to write down the solutions. Most problems should have a short solution (and you can refer to results we have learned about to shorten your solution). Don't repeat calculations that we did in the class unnecessarily.

\vspace{0.3cm}

\textbf{Deadline:} March 12 at 11:55 pm

\newcommand{\cM}{\mathcal{M}}
\newcommand{\nS}{\mathrm{S}}
\newcommand{\nA}{\mathrm{A}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ip}[1]{\langle #1 \rangle}


\section*{Average vs. mixed policies}
Fix policies $\pi^{(1)},\dots,\pi^{(k)}$ of some finite discounted MDP $M=(\cS,\cA,P,r,\gamma)$.
There are two ways of combining these policies with
some weights $\alpha\in \cM_1([k])$.
The first way is to choose one of the policies at random from the multinomial parameterized by $\alpha$
and then follow the resulting policy for all the time steps.
Formally, one would choose
an index $I\in [k]$ at random such that $\Prob{I=i} = \alpha_i$
and then follow the policy $\pi^{(I)}$ for whichever state one encounters.
The second way is to choose the policy to follow at random in each time step.
Call the policy that is obtained following the first method the ($\alpha$-weighted) \textbf{mixture of $\pi^{(1)},\dots,\pi^{(k)}$}.
Call the policy that is obtained following the second method the ($\alpha$-weighted)
\textbf{average of $\pi^{(1)},\dots,\pi^{(k)}$}.

Intuitively,
a distribution $\mu\in \cM_1(\cS)$ over the states and
the interconnection of a mixture policy and $M$ gives rise to a probability space $(\Omega,\cF,\PP)$ that carries the random elements
$I,S_0,A_0,S_1,A_1,\dots$ with $I\in [k]$, $S_t\in \cS$ and $A_t\in \cA$ for $t\ge 0$ and such that
for $H_t = (S_0,A_0,S_1,\dots,A_{t-1},S_t)$,
\begin{enumerate}
\item  $\mathbb{P}(S_0 = s|I) = \mu(s)$ for all $s \in \mathcal{S}$,
\item  $\mathbb{P}(A_t = a | I,H_t)
			= \pi^{(I)}_t(a | H_t)$ for all $a \in \mathcal{A}, t \geq 0$,
\item  $\mathbb{P}(S_{t+1} = s' | I, H_t, A_t) = P_{A_t}(S_t, s')$ for all $s' \in \mathcal{S}$, and
\item $\PP(I=i)=\alpha_i$ for all $i\in [k]$.
\end{enumerate}
Note that all first three criteria are modified to express that the laws that govern $S_0$, the action distribution and the next state distribution are as before even when conditioning on $I$.
A new, fourth criterion is added that expresses that
the distribution of $I$ follows the multinomial distribution with parameter $\alpha$.
That the probability distribution $\PP$ with the above properties
exists is guaranteed again by the Ianescu-Tulcea theorem.
As usual, when needed, we use $\PP_\mu$ to indicate the dependence of $\PP$ on $\mu$.

\newcommand{\N}{\mathbb{N}}
\newcommand{\cG}{\mathcal{G}}

Finally some notation:
For a probability measure $\PP$ on a measurable space $(\Omega,\cF)$ and a sub-sigma algebra $\cG$ of $\cF$, let $\PP|_{\cG}$ be the probability measure on $(\Omega,\cG)$ obtained from $\PP$ by restricting it to $\cG$: $\PP|_{\cG}(U) = \PP(U)$ for any $U\in \cG$.

\begin{question}
Unless otherwise specified let
 $\pi^{(1)},\dots,\pi^{(k)}$ be arbitrary policies of $M$ and let $\alpha\in \cM_1([k])$, $\mu \in \cM_1(\cS)$ be also arbitrary.
 Also, let $(\Omega,\cF,\PP)$ as above (we shall also use $\PP_\mu$ when the dependence on $\mu$ is important).
 Let
 $Z = (S_0,A_0,S_1,A_1,\dots)$.
Show that the following hold:
\begin{enumerate}
\item $Z$ is random element between $(\Omega,\cF)$ and $((\cS\times \cA)^{\N},\cG')$ where $\cG'$ is the product $\sigma$-algebra on $(\cS\times \cA)^{\N}$
induced by the discrete topology on $\cS \times \cA$.
\points{5}
\item
\label{q1:a3:1}
Show that there is a policy $\bar \pi$ of the MDP $M$ such that for any $\mu \in \cM_1(\cS)$,
the pushforward of $\PP_\mu$ under $Z$, $(\PP_\mu)_Z$ satisfies
\[
(\PP_\mu)_Z =\PP_\mu^{\bar \pi}
\]
where $\PP_\mu^{\bar \pi}$ is the unique probability measure on
the canonical space $((\cS\times \cA)^{\N},\cG')$
induced
by the interconnection of  $\bar \pi$ and the MDP, given the initial state distribution $\mu$.
That is, a mixture policy induces a policy $\bar \pi$ of the MDP $M$.
\points{20}
\item
Let $R=\sum_{t=0}^\infty \gamma^t r_{A_t}(S_t)$ and
let $\PP$ be as above with the choice $\mu= \delta_s$. Let $\E$ be the expectation operator
corresponding to $\PP$.
Show that $v(s)=\E[R]$ is well-defined:
That is,
for any $(\Omega,\cF,\PP)$ and $(\Omega,\cF,\PP')$ as long as $\PP$ and $\PP'$ satisfy the above four properties, $\E[R]=\E'[R]$ where $\E'$ is the expectation operator underlying $\PP'$.
\points{10}
\item
Show that $v(s) = v^{\bar \pi}(s)$.
\points{5}
\item Let $\PP_\mu^{\pi^{(i)}}$ ($\PP_{\mu}^{\bar \pi}$) be the
probability measures induced on the canonical space
$((\cS \times \cA)^{\N},\cG')$ by the initial state distribution $\mu$ and the interconnection of
$\pi^{(i)}$ (respectively, $\bar \pi$) with the MDP $M$. Show that
$\PP_{\mu}^{\bar \pi} = \sum_{i=1}^k \alpha_i \PP_{\mu}^{\pi^{(i)}}$.
\points{10}
\item Mixing is guaranteed to keep performance bounds:
if for some
$v:\cS \to \R$ and for all $i\in [k]$,
$v^{\pi^{(i)}}\ge v$ then $v^{\bar \pi}\ge v$.
\points{5}
\item Averaging is not guaranteed to keep performance bounds:
For any $\gamma>1/2$
there exists an MDP with state space $\cS$, $k\ge 2$, policies $\pi_1,\dots,\pi_k$, a function $v:\cS \to \R$ and $\alpha\in \cM_1([k])$ such that $v^{\pi_i}\ge v$ holds for all $i\in [k]$, yet if $\pi$ is the $\alpha$-average of $\pi_1,\dots,\pi_k$ then $v^\pi<v$.
\points{10}
\end{enumerate}
\hint
Recall the change-of-variables formula:
For a random element $X$ taking values in some measurable set $\cX$,
the pushforward $\PP_X$ of $X$ satisfies
\begin{align*}
\EE{ f(X) } = \int f(x) \PP_X(dx)\,.
\end{align*}
Recall also that integration is linear in measures.
In particular,
for any measures $\PP_i$ and nonnegative coefficients $\alpha_i$, $i\in[k]$
and $f$ which is $(\sum_{i=1}^k \alpha \PP_i)$-integrable,
 $\int f d(\sum_{i=1}^k \alpha \PP_i) = \sum_{i=1}^k \alpha_i \int f d\PP_i$
 (this also extends to signed measures, but we won't need this extension).
\tpoints{}
\end{question}



\section*{Finding needles with high probability}

The high-probability needle lemma is as follows:
\begin{lemma}[High-probability needle lemma]
\label{lem:hpn}
Any algorithm that
correctly identifies the single nonzero entry in any binary array of length $k$
with probability at least $0.91$
has the property that
on some input
the expected number of queries that the algorithm uses is
at least $\Omega(k)$.
\end{lemma}

\begin{question}
Prove~\cref{lem:hpn}.
Note that the algorithms are allowed to randomize.
\tpoints{30}
\end{question}



\subsection*{Fitted Value Iteration}
Assume that the rewards belong to the $[0,1]$ interval and fix the discount factor $\gamma$. Let $H_\gamma = 1/(1-\gamma)$.
Assume we are given a feature map $\phi: \cS \times \cA \to \R^d$ which spans $\R^d$.
Let $\cF = \{ f_\theta \,:\, 
f_\theta(s,a) = \phi(s,a)^\top \theta, \theta \in \R^d \}$ be the span of the features.
Let $C \subset \cZ:=\cS \times \cA$ be the set whose existence is guaranteed by the Kiefer-Wolfowitz theorem for the feature map $\phi$ and let $\rho: C \to [0,1]$ be the corresponding weighting function. In particular, $|C|\le d(d+1)/2$, $\sum_{z\in C} \rho(z)=1$ and with $G_\rho = \sum_{z\in C} \rho(z) \phi(z)\phi(z)^\top$, $\max_{z\in \cZ} \norm{\phi(z)}_{G_\rho^{-1}}\le \sqrt{d}$.

For $k\ge 1$, $(s,a)\in \cS \times \cA$, let $C_k(s,a) = [S_1'(k,s,a),\dots,S_m'(k,s,a)]$ be so that all the $(C_k(s,a))_{k,s,a}$ are independent of each other, and for any $k,s,a$, $S_1'(k,s,a),\dots,S_m'(k,s,a) \stackrel{\textrm{iid}}{\sim} P_a(s)$.
For $k\ge 1$ let $\hat T_k: \R^{\cS\times \cA} \to \R$ be defined by
\begin{align*}
(\hat T_k q)(s,a) = r_a(s) + \frac{\gamma}{m} \sum_{s'\in C_k(s,a)} Mq \, (s')\,.
\end{align*}
Further, let $\Pi: \R^{\cZ} \to \R^{\cZ}$ be defined by $(\Pi f)(z) = \max(\min(f(z),H_\gamma),0)$: In words, $\Pi$ truncates the values of its argument to the $[0,H_\gamma]$ interval.

Consider the following procedure, which we call fitted $q$ iteration (FQI).\footnote{A terrible name.}
\begin{enumerate}
\item $\theta_0 = \0$
\item {\tt for } $k=1,2,\dots,K$ {\tt do}
\item $\qquad$ $\theta_k = \argmin_{\theta\in \R^d} \sum_{z\in C} \rho(z) (f_\theta(z)-(\hat T_k \Pi f_{\theta_{k-1}})(z))^2$
\item {\tt return} $\theta_K$
\end{enumerate}

\newcommand{\epx}{\varepsilon_{\textrm{apx}}}
Let $\epx = \sup_{\theta} \inf_{\theta'} \norm{  f_{\theta'} - T \Pi f_{\theta} }_\infty$.
\begin{question}
Prove that the following hold:
\begin{enumerate}
\item The computation cost of FQI is $O(K d^3 m A)$ and it needs $O(d^2)$ space (all in the \href{https://en.wikipedia.org/wiki/Random-access_machine}{RAM model of computation}). The query cost is $O(K d^2 m)$. Explain how you get the bounds.
\points{5}
\item Fix $k\ge 0$. 
Let $q_k = \Pi f_{\theta_k}$. For $k>0$, let $\epsilon_k:\cZ \to R$ and $\theta_k^*\in \R^d$ 
be such that 
$T q_{k-1}= f_{\theta_k^*}+\epsilon_k$ 
and $\norm{\epsilon_k}_\infty \le \epx$. Show that $\epsilon_k$ and $\theta_k^*$ are well-defined (i.e., they exist).
\points{10}
\item Show that for any  $k\ge 1$, $0\le \zeta\le 1$, with probability at least $1-\zeta$,
\begin{align*}
\norm{q_k - T q_{k-1}}_\infty \le (1+\sqrt{d}) \epx + \sqrt{d} H_\gamma \sqrt{\frac{\log\left(\frac{2|C|}{\zeta}\right)}{2m}}\,.
\end{align*}
\points{10}
\item Show that, on the same event as in the previous part, the policy $\pi$ that is greedy with respect to $q_K$ is $\delta$-optimal with 
\begin{align*}
\delta \le 2 H_\gamma^2
\left\{
(1+\sqrt{d}) \epx +
 \gamma^K 
+ \sqrt{d} H_\gamma \sqrt{\frac{\log\left(\frac{2|C| K}{\zeta}\right)}{2m}} \right\}\,.
\end{align*}
\points{10}
\item 
Fix $\epsilon>0$.
Argue that $K$, $m$ and $\zeta$ can be chosen
as a polynomial function of $H_\gamma,d, 1/\epsilon$
so that the {\emph expected} suboptimality of the policy $\pi$ is bounded by
$2H_\gamma^2 (1+\sqrt{d})\epx + 2\epsilon$.  Show the choices you made.
\points{5}
\item Argue that with a query, runtime and space cost that is polynomial in $H_\gamma,d, 1/\epsilon,A$, 
the procedure obtains a policy $\pi$ that is at most $\delta$-optimal with
$\delta=2H_\gamma^2 (1+\sqrt{d})\epx + 2\epsilon$.
\points{5}
\item The MDP $\cM = (\cS,\cA,P,r,\gamma)$ is called linear in $\phi$ if it holds that with some $\theta_r\in \R^d$, $r_a(s) = f_{\theta_r}(s,a)$ holds for all $(s,a)$ and if for some $\mu:\cS \to \R^d$, 
 for any $(s,a)$,
$P_a(s,s') = \ip{\phi(s,a), \mu(s')}$. Show that if $\cM$ is linear in $\phi$ then $\epx = 0$.
\points{10}
\end{enumerate}
\tpoints{}
\end{question}





\bigskip
\bigskip

\noindent
\textbf{
Total for all questions: \arabic{DocPoints}}.
Of this, $30$ are bonus marks (i.e., $120$ marks worth $100\%$ on this problem set).

\end{document}

